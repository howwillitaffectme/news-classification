{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1988764-6826-495f-b6be-ee321951154b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Text classification for fake news detection\n",
    "\n",
    "In this notebook I train and evaluate 3 different ML models from sklearn library, namely:\n",
    "- Passive Aggressive Classifier\n",
    "- Logistic Regression\n",
    "- Linear SVC\n",
    "\n",
    "All three models use TF-iDF vectorizer, a frequency based textvectorizer.\n",
    "The models will classify news based only on the text or title of the news.\n",
    "\n",
    "The investigation is structured in the following manner:\n",
    "1. Read and preprocess data.\n",
    "2. Split data into training and test sets, vectorize it for models input.\n",
    "3. Training and evaluation of models.\n",
    "4. Summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaaf646-485a-474f-bfdc-9749cb598d1a",
   "metadata": {},
   "source": [
    "Download dataset: https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset and extract into the same folder this notebook is in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d1f0b1f-53be-4ee9-b273-8dea60e44cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "# text processing, term frequency based\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "# models\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier, LogisticRegression\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be85617a-25e5-430a-a808-34d03d530a41",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "\n",
    "The input data is not 'clean'. Apperently many \"true\" news contain a source in the begining of their \"text\" field, and many \"fake\" news contain related pic info in the end of their \"text\" field, this can cause bias for a model, so we can cut them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59522b62-1bb8-41ae-a841-5fdb23237ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutsource(s):\n",
    "    ''' a function to cut out news source in \"true\" texts\n",
    "        luckily they are separated by '-' (dash sign)\n",
    "    '''\n",
    "    if '- ' in s:\n",
    "        s1 = s.split('- ')[0]\n",
    "        s = s[len(s1)+2:]\n",
    "        \n",
    "    return s\n",
    "\n",
    "def cutgetty(s):\n",
    "    ''' a function to cut out 'getty images' in \"fake\" texts\n",
    "    '''\n",
    "    s = re.sub('Getty Images', '', s)\n",
    "    \n",
    "    return s\n",
    "\n",
    "def cutfactbox(s):\n",
    "    ''' a function to cut out 'factbox' in \"true\" titles\n",
    "    '''\n",
    "    s = re.sub('factbox', '', s, flags=re.IGNORECASE)\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55981c0b-97fe-42e5-84f1-0bad6c12ef3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = pd.read_csv('Fake.csv')\n",
    "true = pd.read_csv('True.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03180ebf-19cb-49c9-9686-0ba7f7ac0f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leaked Email Proves Trump Officials Aware Russia Had ‘Thrown The USA Election’ To Trump\n",
      "Donald Trump s current deputy national security adviser K.T. McFarland, a former Fox News personality, K. T. McFarland admitted in an email to a colleague during the 2016 presidential transition to Russia throwing the election to Trump. The leaked email was written just weeks before Trump s inauguration and it states that sanctions would make it difficult to ease relations with Russia,  which has just thrown the U.S.A. election to him. The New York Times reports:But emails among top transition officials, provided or described to The New York Times, suggest that Mr. Flynn was far from a rogue actor. In fact, the emails, coupled with interviews and court documents filed on Friday, showed that Mr. Flynn was in close touch with other senior members of the Trump transition team both before and after he spoke with the Russian ambassador, Sergey I. Kislyak, about American sanctions against Russia.A White House lawyer tried to explain McFarland s email to the The Times by saying that she was referring to the Democrats  portrayal of the election. That doesn t make any sense, by the way.McFarland wrote the email to Thomas P. Bossert, who currently serves as Trump s homeland security adviser, then he forwarded it to future National Security Advisor Michael Flynn (now indicted), future Chief of Staff Reince Priebus, future senior strategist Stephen Bannon, and future press secretary Sean Spicer, the Daily Beast reports.With all the pearl-clutching we witnessed from conservatives about Hillary Clinton s emails, you d think they wouldn t be sending messages about Russia throwing the election to Trump.This past March, John Oliver, the host of the HBO comedy show Last Week Tonight started a segment called  Stupid Watergate,  which he described as  a scandal with all the potential ramifications of Watergate, but where everyone involved is stupid and bad at everything. Nailed it!Photo by Chip Somodevilla/.\n",
      "fake\n"
     ]
    }
   ],
   "source": [
    "true['text'] = true['text'].apply(cutsource)\n",
    "fake['text'] = fake['text'].apply(cutgetty)\n",
    "true['title'] = true['title'].apply(cutfactbox)\n",
    "# combine data into 1 dataframe, discarding 'date' and 'subject' fields, \n",
    "# removing rows with empty text or title.\n",
    "cols = ['title', 'text']\n",
    "df = pd.concat([fake[cols], true[cols]], ignore_index=True)\n",
    "df['text'] = df['text'].str.strip()\n",
    "df['title'] = df['title'].str.strip()\n",
    "label = len(fake)*['fake'] + len(true)*['true']\n",
    "df['label'] = label\n",
    "# drop news shorter than a tweet\n",
    "df = df[df['text'].str.len() > 280]\n",
    "df = df.replace('', np.nan)\n",
    "df.dropna(inplace=True)\n",
    "df['label'].value_counts()\n",
    "example = df.iloc[42]\n",
    "print(example['title'] + '\\n' + example['text'] + '\\n' + example['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412d4020-76f5-4e0f-9a8e-889bf330bbd0",
   "metadata": {},
   "source": [
    "### Machine learning time. \n",
    "Training will consider 2 cases: title only and text only classification.\n",
    "\n",
    "For word processing I use TF-iDF, a frequency based metric, which checks the occurence of a term against a given text and the whole corpus.\n",
    "\n",
    "Here I construct a function for evaluation of different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cf59cc9-7063-4a1e-b9ce-c6e511fad000",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(df_col):\n",
    "    '''split data into train and test, turn into vectors.\n",
    "    '''\n",
    "    x_train,x_test,y_train,y_test=train_test_split(df[df_col], df['label'], test_size=0.2, random_state=42, shuffle=True)\n",
    "    # Learn vocabulary and idf, return document-term matrix.\n",
    "    tfidf_vectorizer=TfidfVectorizer(stop_words='english', max_df=0.75)\n",
    "    vec_train=tfidf_vectorizer.fit_transform(x_train.values.astype('U')) \n",
    "    # Transform documents to document-term matrix.\n",
    "    vec_test=tfidf_vectorizer.transform(x_test.values.astype('U'))\n",
    "    \n",
    "    return (tfidf_vectorizer, vec_train, vec_test, y_train, y_test)\n",
    "    \n",
    "def model_eval(input_data, model):\n",
    "    '''function to report f1 scores for a model\n",
    "        based on classification based on df_col (text or title)\n",
    "        tdidf_vectorizer\n",
    "        https://github.com/satssehgal/FakeNewsDetector\n",
    "    '''\n",
    "    tfidf_vectorizer, vec_train, vec_test, y_train, y_test = input_data\n",
    "    model.fit(vec_train,y_train)\n",
    "    y_pred=model.predict(vec_test)\n",
    "    f1 = f1_score(y_test, y_pred, pos_label='fake')\n",
    "    # let's see what are the top terms for fake and true news (largest vectors)\n",
    "    # https://www.datacamp.com/community/tutorials/scikit-learn-fake-news\n",
    "    terms = tfidf_vectorizer.get_feature_names_out()\n",
    "    keywords = sorted(zip(model.coef_[0], terms), reverse=True)\n",
    "    keywords_true = np.array(keywords[:20])\n",
    "    keywords_fake = np.flip(np.array(keywords[-20:]), axis=0)\n",
    "    true_out = keywords_true[:, 1]\n",
    "    fake_out = keywords_fake[:, 1]\n",
    "\n",
    "    print('F1 score', f1)\n",
    "    print('\\t True keywords: \\n', true_out)\n",
    "    print('\\t Fake keywords: \\n', fake_out)\n",
    "    \n",
    "    return (f1, true_out, fake_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80fe08ea-be91-451e-b3ef-979f3a72c392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*--------- PassiveAggressiveClassifier ---------*\n",
      "## Title based classification ##\n",
      "F1 score 0.9376854599406528\n",
      "\t True keywords: \n",
      " ['says' 'exclusive' 'faults' 'spokesman' 'fame' 'kremlin' 'rohingya'\n",
      " 'hindu' 'german' 'kabul' 'blitz' 'urges' 'talks' 'pakistan' 'myanmar'\n",
      " 'employs' 'fights' 'vulgar' 'collar' 'weigh']\n",
      "\t Fake keywords: \n",
      " ['video' 'breaking' 'racist' 'gop' 'just' 'joe' 'hillary' 'illegals'\n",
      " 'james' 'anonymous' 'actually' 'watch' 'gitmo' 'ck' 'ammo' 'globalist'\n",
      " 'dog' 'dems' 'destroy' 'knees']\n",
      "## Text based classification ##\n",
      "F1 score 0.9875528148909445\n",
      "\t True keywords: \n",
      " ['thursday' 'tuesday' 'wednesday' 'nov' 'friday' 'republican' 'reuters'\n",
      " 'monday' 'donald' 'spokeswoman' 'rival' 'spokesman' 'statement' 'comment'\n",
      " 'referring' 'representatives' 'barack' 'reporters' 'saying' 'edt']\n",
      "\t Fake keywords: \n",
      " ['featured' 'image' 'read' 'gop' 'just' 'com' 'sen' 'rep' 'watch' 'pic'\n",
      " 'breitbart' 'wfb' 'mr' 'https' 'daily' 'hillary' 'mail' 'wire'\n",
      " 'reportedly' '21st']\n",
      "*--------- LogisticRegression ---------*\n",
      "## Title based classification ##\n",
      "F1 score 0.9417195614541258\n",
      "\t True keywords: \n",
      " ['says' 'china' 'house' 'talks' 'myanmar' 'senate' 'urges' 'german'\n",
      " 'south' 'turkey' 'pm' 'britain' 'lawmakers' 'brexit' 'tax' 'eu' 'ex'\n",
      " 'probe' 'exclusive' 'opposition']\n",
      "\t Fake keywords: \n",
      " ['video' 'hillary' 'watch' 'breaking' 'just' 'gop' 'america' 'tweets'\n",
      " 'muslim' 'racist' 'donald' 'details' 'bernie' 'obama' 'boiler' 'isis'\n",
      " 'wow' 'black' 'images' 'liberal']\n",
      "## Text based classification ##\n",
      "F1 score 0.9764989109251404\n",
      "\t True keywords: \n",
      " ['wednesday' 'reuters' 'thursday' 'tuesday' 'republican' 'friday' 'monday'\n",
      " 'spokesman' 'statement' 'nov' 'presidential' 'reporters' 'minister'\n",
      " 'comment' 'democratic' 'told' 'representatives' 'spokeswoman' 'sunday'\n",
      " 'edt']\n",
      "\t Fake keywords: \n",
      " ['image' 'featured' 'just' 'read' 'gop' 'com' 'hillary' 'watch' 'america'\n",
      " 'pic' 'mr' 'like' 'https' 'sen' 'video' 'daily' 'rep' 'american' 'fact'\n",
      " 'wire']\n",
      "*--------- LinearSVC ---------*\n",
      "## Title based classification ##\n",
      "F1 score 0.9514119126557676\n",
      "\t True keywords: \n",
      " ['says' 'britain' 'german' 'urges' 'myanmar' 'rohingya' 'exclusive'\n",
      " 'spokesman' 'turkey' 'talks' 'singapore' 'pakistan' 'kremlin' 'house'\n",
      " 'faults' 'colombia' 'indonesian' 'spain' 'negotiations' 'egypt']\n",
      "\t Fake keywords: \n",
      " ['video' 'breaking' 'gop' 'just' 'hillary' 'watch' 'racist' 'bernie'\n",
      " 'boiler' 'illegals' 'isis' 'ck' 'actually' 'doj' 'dem' 'shocking' 'james'\n",
      " 'sharia' 'joe' 'anonymous']\n",
      "## Text based classification ##\n",
      "F1 score 0.9880013712718547\n",
      "\t True keywords: \n",
      " ['thursday' 'wednesday' 'tuesday' 'reuters' 'nov' 'friday' 'monday'\n",
      " 'republican' 'statement' 'spokesman' 'comment' 'spokeswoman' 'reporters'\n",
      " 'representatives' 'barack' 'donald' 'rival' 'presidential' 'referring'\n",
      " 'sunday']\n",
      "\t Fake keywords: \n",
      " ['featured' 'image' 'read' 'gop' 'just' 'com' 'sen' 'watch' 'pic'\n",
      " 'breitbart' 'rep' 'mr' 'daily' 'hillary' 'wfb' 'https' 'wire'\n",
      " 'reportedly' 'mail' '21st']\n"
     ]
    }
   ],
   "source": [
    "# split data into training and test sets, vectorize.\n",
    "title_data = data_split('title')\n",
    "text_data = data_split('text')\n",
    "\n",
    "# setting models and computing scores\n",
    "pac = PassiveAggressiveClassifier()\n",
    "lr = LogisticRegression()\n",
    "lsvc = LinearSVC()\n",
    "table = []\n",
    "for model in (pac, lr, lsvc):\n",
    "    print('*--------- '+type(model).__name__+' ---------*')\n",
    "    print('## Title based classification ##')\n",
    "    f1, true_m, fake_m = model_eval(title_data, model)\n",
    "    print('## Text based classification ##') \n",
    "    f1txt, true_mtxt, fake_mtxt = model_eval(text_data, model)\n",
    "    table.append([f1, true_m[:5], fake_m[:5], f1txt, true_mtxt[:5], fake_mtxt[:5]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42f71193-986d-440e-8ab8-8d22763828f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1 score (title)</th>\n",
       "      <th>Top true (title)</th>\n",
       "      <th>Top fake (title)</th>\n",
       "      <th>F1 score (text)</th>\n",
       "      <th>Top true (text)</th>\n",
       "      <th>Top fake (text)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PAC</th>\n",
       "      <td>0.937685</td>\n",
       "      <td>[says, exclusive, faults, spokesman, fame]</td>\n",
       "      <td>[video, breaking, racist, gop, just]</td>\n",
       "      <td>0.987553</td>\n",
       "      <td>[thursday, tuesday, wednesday, nov, friday]</td>\n",
       "      <td>[featured, image, read, gop, just]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogReg</th>\n",
       "      <td>0.941720</td>\n",
       "      <td>[says, china, house, talks, myanmar]</td>\n",
       "      <td>[video, hillary, watch, breaking, just]</td>\n",
       "      <td>0.976499</td>\n",
       "      <td>[wednesday, reuters, thursday, tuesday, republ...</td>\n",
       "      <td>[image, featured, just, read, gop]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinSVC</th>\n",
       "      <td>0.951412</td>\n",
       "      <td>[says, britain, german, urges, myanmar]</td>\n",
       "      <td>[video, breaking, gop, just, hillary]</td>\n",
       "      <td>0.988001</td>\n",
       "      <td>[thursday, wednesday, tuesday, reuters, nov]</td>\n",
       "      <td>[featured, image, read, gop, just]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        F1 score (title)                            Top true (title)  \\\n",
       "PAC             0.937685  [says, exclusive, faults, spokesman, fame]   \n",
       "LogReg          0.941720        [says, china, house, talks, myanmar]   \n",
       "LinSVC          0.951412     [says, britain, german, urges, myanmar]   \n",
       "\n",
       "                               Top fake (title)  F1 score (text)  \\\n",
       "PAC        [video, breaking, racist, gop, just]         0.987553   \n",
       "LogReg  [video, hillary, watch, breaking, just]         0.976499   \n",
       "LinSVC    [video, breaking, gop, just, hillary]         0.988001   \n",
       "\n",
       "                                          Top true (text)  \\\n",
       "PAC           [thursday, tuesday, wednesday, nov, friday]   \n",
       "LogReg  [wednesday, reuters, thursday, tuesday, republ...   \n",
       "LinSVC       [thursday, wednesday, tuesday, reuters, nov]   \n",
       "\n",
       "                           Top fake (text)  \n",
       "PAC     [featured, image, read, gop, just]  \n",
       "LogReg  [image, featured, just, read, gop]  \n",
       "LinSVC  [featured, image, read, gop, just]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_ev = pd.DataFrame(table, index=['PAC', 'LogReg', 'LinSVC'], \n",
    "                         columns=['F1 score (title)', 'Top true (title)', 'Top fake (title)', \n",
    "                                 'F1 score (text)', 'Top true (text)', 'Top fake (text)'])\n",
    "models_ev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e862df6-36ea-4010-9e4b-cf7447276feb",
   "metadata": {},
   "source": [
    "NOTE: because of shuffle of input data the keywords might differ from run to run (e.g. starting a new kernel), I think the seed for random_state in train_test_split() is not uniquely determined for a given number.\n",
    "\n",
    "### Summary\n",
    "From the textbased classification we can conclude that the models identified true news as the ones that refer to to a person or other news, like 'said', 'showed', 'citing', 'comment' as well as containing a date (day).\n",
    "On the other hand, fake news seem to refer to images a lot (presumably in the news article) or links on the internet (e.g. pic.twitter.com/*). Also in contrast true news refer to particular days, while fake news refer to rather time adverbs, like 'just', 'daily', 'reportedly'.\n",
    "Alternatively fake news refer to rather screenshots than text citations.\n",
    "\n",
    "It is also plausible that fake news appeal to visual comprehension of information in contrast to true news appealing to verbal/idea based information. \n",
    "\n",
    "The title based classification shows that keywords like \"shocking\", \"breaking\", \"racist\" were identified for fake news. Fake news are similar to a virus, they are aimed at getting as many clicks as possible in short amount of time, indeed the  keywords for fake news look like clickbaiting words. While true news seem to have more neutral and passive keywords. \n",
    "\n",
    "Within this investigation all models show similar high F1 scores, with PAC and LogReg being slightly better than LogReg most of the times. At the same time textbased modelling shows higher scores than titlebased, which is reasonable, the more information is available, the more precise one can be (to some extent).\n",
    "\n",
    "We can conclude that all the models showed reasonable results which we can also interpret in a clear understandable manner.\n",
    "\n",
    "PAC was my first choice since I didn't really know what to pick, then I took LogisiticRegression for it being the simplest and easiest to understand, and  SVC as google said is one of the best for text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629eee31-ac67-4692-a137-116e2b5f8630",
   "metadata": {},
   "source": [
    "## CNN (Convolutional Neural Network) for text classification using PyTorch\n",
    "\n",
    "This part of investigation is an attempt to apply the CNN classification algorithm written for movie reviews [[2]](#references). \n",
    "\n",
    "I'll go through the following steps:\n",
    "1. Text preprocessing\n",
    "    - Embedding layer and Tokenization\n",
    "    - Padding\n",
    "2. CNN model\n",
    "3. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721f0fc1-ed75-40ce-bfc6-406e9f660bf2",
   "metadata": {},
   "source": [
    "### Text preprocessing\n",
    "\n",
    "I am going to use an embedding pre-trained layer to which I'd like to map words in the news. For that first I need to tokenize the news, i.e. turn them into lists of words, tokens and then map them to the embedding pre-trained layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dea97ec-8f00-4cae-9fb3-ff47ec5ddc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase the texts and remove punctuation\n",
    "df['text'] = df['text'].str.lower()\n",
    "df['text'] = df['text'].str.replace('[^\\w\\s]','', regex=True) # remove punctuation (everything that's not a word(also a number) or whitespace)\n",
    "texts_split = df['text'].str.split().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "930d4cd5-0e08-45bd-a598-151922807a4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['donald',\n",
       " 'trump',\n",
       " 's',\n",
       " 'current',\n",
       " 'deputy',\n",
       " 'national',\n",
       " 'security',\n",
       " 'adviser',\n",
       " 'kt',\n",
       " 'mcfarland',\n",
       " 'a',\n",
       " 'former',\n",
       " 'fox',\n",
       " 'news',\n",
       " 'personality']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now the texts are lists of words\n",
    "texts_split[42][:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ae67e8d-9021-489a-98a3-17a5155aeb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seems like I have 1 letter words which usually don't carry much semantic significance\n",
    "# let's get rid of them\n",
    "for i, text in enumerate(texts_split):\n",
    "    texts_split[i] = [word for word in text if len(word)>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d5bc1f6-b16d-4aba-9c38-c46e97c51b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['donald', 'trump', 'current', 'deputy', 'national', 'security', 'adviser', 'kt', 'mcfarland', 'former', 'fox', 'news', 'personality', 'mcfarland', 'admitted', 'in', 'an', 'email', 'to', 'colleague', 'during', 'the', '2016', 'presidential', 'transition', 'to', 'russia', 'throwing', 'the', 'election', 'to', 'trump', 'the', 'leaked', 'email', 'was', 'written', 'just', 'weeks', 'before', 'trump', 'inauguration', 'and', 'it', 'states', 'that', 'sanctions', 'would', 'make', 'it', 'difficult', 'to', 'ease', 'relations', 'with', 'russia', 'which', 'has', 'just', 'thrown', 'the', 'usa', 'election', 'to', 'him', 'the', 'new', 'york', 'times', 'reportsbut', 'emails', 'among', 'top', 'transition', 'officials', 'provided', 'or', 'described', 'to', 'the', 'new', 'york', 'times', 'suggest', 'that', 'mr', 'flynn', 'was', 'far', 'from', 'rogue', 'actor', 'in', 'fact', 'the', 'emails', 'coupled', 'with', 'interviews', 'and', 'court', 'documents', 'filed', 'on', 'friday', 'showed', 'that', 'mr', 'flynn', 'was', 'in', 'close', 'touch', 'with', 'other', 'senior', 'members', 'of', 'the', 'trump', 'transition', 'team', 'both', 'before', 'and', 'after', 'he', 'spoke', 'with', 'the', 'russian', 'ambassador', 'sergey', 'kislyak', 'about', 'american', 'sanctions', 'against', 'russiaa', 'white', 'house', 'lawyer', 'tried', 'to', 'explain', 'mcfarland', 'email', 'to', 'the', 'the', 'times', 'by', 'saying', 'that', 'she', 'was', 'referring', 'to', 'the', 'democrats', 'portrayal', 'of', 'the', 'election', 'that', 'doesn', 'make', 'any', 'sense', 'by', 'the', 'waymcfarland', 'wrote', 'the', 'email', 'to', 'thomas', 'bossert', 'who', 'currently', 'serves', 'as', 'trump', 'homeland', 'security', 'adviser', 'then', 'he', 'forwarded', 'it', 'to', 'future', 'national', 'security', 'advisor', 'michael', 'flynn', 'now', 'indicted', 'future', 'chief', 'of', 'staff', 'reince', 'priebus', 'future', 'senior', 'strategist', 'stephen', 'bannon', 'and', 'future', 'press', 'secretary', 'sean', 'spicer', 'the', 'daily', 'beast', 'reportswith', 'all', 'the', 'pearlclutching', 'we', 'witnessed', 'from', 'conservatives', 'about', 'hillary', 'clinton', 'emails', 'you', 'think', 'they', 'wouldn', 'be', 'sending', 'messages', 'about', 'russia', 'throwing', 'the', 'election', 'to', 'trumpthis', 'past', 'march', 'john', 'oliver', 'the', 'host', 'of', 'the', 'hbo', 'comedy', 'show', 'last', 'week', 'tonight', 'started', 'segment', 'called', 'stupid', 'watergate', 'which', 'he', 'described', 'as', 'scandal', 'with', 'all', 'the', 'potential', 'ramifications', 'of', 'watergate', 'but', 'where', 'everyone', 'involved', 'is', 'stupid', 'and', 'bad', 'at', 'everything', 'nailed', 'itphoto', 'by', 'chip', 'somodevilla']\n"
     ]
    }
   ],
   "source": [
    "print(texts_split[42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ae2b366-e14c-409b-b06e-18a9b38a98c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean and median number of words in texts: 404.93019340052854 & 356.0\n"
     ]
    }
   ],
   "source": [
    "# one last preprocessing step is that I would like to cut the length of the texts to have a \"light\" model\n",
    "lens = [len(text) for text in texts_split]\n",
    "print(\"mean and median number of words in texts:\", np.mean(lens), '&', np.median(lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f2f899d-884c-4fe4-92af-304bd3832bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I push this number up to 400 and cut all the texts up to this number\n",
    "max_len = 400\n",
    "for i, text in enumerate(texts_split):\n",
    "    texts_split[i] = text[:max_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb75a6ad-0a8d-43a7-b9f2-aafa6694660f",
   "metadata": {},
   "source": [
    "#### Embedding layer and Tokenization\n",
    "You will need to download word2vec model ```GoogleNews-vectors-negative300-SLIM.bin.gz``` (approx. 300MB) from\n",
    "\n",
    "https://github.com/eyaler/word2vec-slim/blob/master/GoogleNews-vectors-negative300-SLIM.bin.gz\n",
    "\n",
    "and put it into the same folder the project is. \n",
    "\n",
    "This word2vec model was compiled from google news which suits quite well for this project.\n",
    "After loading the model I tokenize the texts according to the loaded word2vec model, i.e. I map the words from the corpus to the integers from lookup table of the model. As an output I have a 2D array of integers represting words in the news, each row is a seperate news text. I also cut long news and left pad short news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ff0ceca-7501-4716-8262-a6fe55d1ebc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Need to run this cell once\n",
    "## unziping our word2vec model \n",
    "# ! gzip -d GoogleNews-vectors-negative300-SLIM.bin.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b5b178f-73fd-41cc-ab38-cf49fb77b42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299567 words in the vocabulary\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# loading the model\n",
    "embed_lookup = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300-SLIM.bin', \n",
    "                                                 binary=True)\n",
    "print(len(embed_lookup), 'words in the vocabulary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "771e7e6e-a421-4f8c-a5a6-a65aae6169b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of embedding:  300\n",
      "Index of ' news ' in the lookup table: 283\n"
     ]
    }
   ],
   "source": [
    "word = 'news'\n",
    "print(\"Length of embedding: \", len(embed_lookup[word]))  # dimension of the vector space of words\n",
    "# embed_lookup.index_to_key[:11]\n",
    "print(\"Index of '\", word, \"' in the lookup table:\", embed_lookup.key_to_index[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eefa96b1-6568-4dda-9eee-79495829ba81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization: For each news text we represent words as their index in the lookup table\n",
    "# unknown words are represnted as 0s, i.e. spaces\n",
    "tokenized_news = []\n",
    "for text in texts_split:\n",
    "    ints = []\n",
    "    for word in text:\n",
    "        try:\n",
    "            idx = embed_lookup.key_to_index[word]\n",
    "        except:\n",
    "            idx = 0\n",
    "        ints.append(idx)\n",
    "    tokenized_news.append(ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "015d00ed-8651-43ab-a8ae-670e081e3681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An example of a tokenized text: \n",
      " [177069, 23049, 378, 2326, 387, 348, 4042, 78551, 0, 234, 24411, 283, 5376, 0, 1790, 0, 24, 2165, 0, 7126, 128, 9, 0, 1874, 3226, 0, 127297, 3215, 9, 600, 0, 23049, 9, 8176, 2165, 8, 1465, 71, 410, 93, 23049, 9432, 0, 13, 713, 2, 3656, 42, 103, 13, 777, 0, 3268, 2090, 6, 127297, 43, 22, 71, 3381, 9, 73282, 600, 0, 88, 9, 60, 65033, 337, 0, 0, 341, 200, 3226, 232, 836, 26, 1280, 0, 9, 60, 65033, 337, 2731, 2, 64499, 0, 8, 330, 15, 14090, 2595, 0, 562, 9, 0, 6749, 6, 3529, 0, 321, 1578, 969, 4, 61858, 753, 2, 64499, 0, 8, 0, 374, 2369, 6, 61, 537, 264, 0, 9, 23049, 3226, 96, 172, 93, 0, 50, 20, 1538, 6, 9, 103824, 4994, 0, 0, 41, 46734, 3656, 97, 0, 1151, 513, 1801, 907, 0, 2873, 0, 2165, 0, 9, 9, 337, 16, 451, 2, 69, 8, 3653, 0, 9, 27631, 15090, 0, 9, 600, 2, 68710, 103, 95, 1131, 16, 9, 0, 1053, 9, 2165, 0, 121330, 0, 28, 664, 2639, 12, 23049, 9475, 348, 4042, 138, 20, 13048, 13, 0, 329, 387, 348, 6319, 88689, 0, 92, 6348, 329, 683, 0, 524, 0, 0, 329, 537, 7289, 142378, 0, 0, 329, 1221, 2114, 122635, 0, 9, 1306, 15434, 0, 47, 9, 0, 34, 5300, 15, 6338, 41, 154550, 118625, 0, 38, 144, 29, 98461, 14, 2320, 3363, 41, 127297, 3215, 9, 600, 0, 0, 246, 5387, 52144, 205488, 9, 1073, 0, 9, 211192, 3799, 245, 59, 110, 1606, 417, 3003, 250, 6536, 252880, 43, 20, 1280, 12, 3982, 6, 47, 9, 617, 15663, 0, 252880, 30, 111, 858, 637, 3, 6536, 0, 675, 10, 806, 13018, 0, 16, 4355, 0]\n"
     ]
    }
   ],
   "source": [
    "print('An example of a tokenized text: \\n', tokenized_news[42])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04aad5dd-6099-4391-9d3e-d897446f9ec2",
   "metadata": {},
   "source": [
    "#### Padding\n",
    "Since I have already cut my texts up to ```max_len``` I will need to left pad with 0s all the texts that are shorter than this number. This will bring all the texts to the same length. I will end up with a 2D array with as many rows as there are news texts and as many columns as ```max_len``` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e7284fe-8155-45b5-8235-ab73064ce695",
   "metadata": {},
   "outputs": [],
   "source": [
    "pttexts = np.zeros((len(tokenized_news), max_len), dtype=int)\n",
    "for i, tok_text in enumerate(tokenized_news):\n",
    "    pttexts[i, -len(tok_text):] = tok_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99c97c4c-81dc-4f39-8d49-632e48a411aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padded and tokenized first 11 texts up to first 10 words \n",
      " [[177069  23049     71  93409   2157     47  69404   1013     60     32]\n",
      " [     0      0      0      0      0      0      0      0      0      0]\n",
      " [     4  61858     13      8   1744      2    234 178446   4053  81995]\n",
      " [     4  68834    107 177069  23049    317      2     20     42     14]\n",
      " [  7705 205961    219     23    647  68834    107   1197      0  24121]\n",
      " [     0      0      0      0      0      0      0      0      0      0]\n",
      " [     0      0      0      0      0      0      0      0      0      0]\n",
      " [     0      0      0      0      0      0      0      0      0      0]\n",
      " [   124     63     19    965      9   6764   1732      9    562      2]\n",
      " [     0      0      0      0      0      0      0      0      0      0]\n",
      " [     0      0      0      0      0      0      0      0      0      0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"padded and tokenized first 11 texts up to first 10 words \\n\", pttexts[:11, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547326c2-a5a4-4978-b9b3-d5e72b49a7a0",
   "metadata": {},
   "source": [
    "### CNN model\n",
    "Now I take the preprocessed data, split it into training, test and validation sets. Then I take the CNN model, load the data and train. \n",
    "\n",
    "***TO BE CONTINUED...***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648dd98d-0c1f-4b41-bc15-34f320dc510d",
   "metadata": {},
   "source": [
    "### References\n",
    "1. https://www.datacamp.com/community/tutorials/scikit-learn-fake-news\n",
    "2. https://cezannec.github.io/CNN_Text_Classification/\n",
    "3. https://github.com/eyaler/word2vec-slim\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
